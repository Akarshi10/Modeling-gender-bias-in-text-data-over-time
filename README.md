# Modeling-gender-bias-in-text-data-over-time

An embedding word is a learned representation for a text where words have a similar meaning. This approach is one of the key breaking points in the deep learning of difficult natural language problems. It represents words and documents. Geometric relationships are used in this paper to capture a meaningful semantic relationship between the words concerned. This integration helps to understand societal changes, for example, how certain adjectives are over time associated with some populations. Thus, a successful intersection between machine and quantitative social science is opened. The subject of modeling is a text-mining tool often used to detect semi-hidden structures in a textual body. In this paper I use topic modelling technique to find the top 10 words in the CORPUS and thus find the top 3 topics of each word.
